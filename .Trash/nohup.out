 * Serving Flask app 'app'
 * Debug mode: on
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://147.8.178.206:5000
[33mPress CTRL+C to quit[0m
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [25/Jul/2023 20:58:58] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 20:59:35] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 20:59:35] "GET /static/wawa_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 20:59:35] "GET /static/wawa_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 20:59:35] "GET /static/wawa_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 21:02:06] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 21:02:15] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4151 tokens (2103 in the messages, 2048 in the completion). Please reduce the length of the messages or completion.
147.8.179.195 - - [25/Jul/2023 21:03:16] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4147 tokens (2099 in the messages, 2048 in the completion). Please reduce the length of the messages or completion.
147.8.179.195 - - [25/Jul/2023 21:03:31] "[33mGET /ckear HTTP/1.1[0m" 404 -
147.8.179.195 - - [25/Jul/2023 21:03:37] "[33mGET /clear HTTP/1.1[0m" 404 -
147.8.179.195 - - [25/Jul/2023 21:04:16] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [25/Jul/2023 21:04:28] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:19:36] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:19:36] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [26/Jul/2023 10:19:48] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:20:20] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:22:49] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:22:49] "GET /static/LungCancer_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:22:49] "GET /static/LungCancer_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:22:49] "GET /static/LungCancer_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 10:57:59] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 11:02:53] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 11:03:30] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 11:05:03] "GET / HTTP/1.1" 200 -
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/config.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.178.199 - - [26/Jul/2023 12:07:37] "GET / HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:07:37] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.178.199 - - [26/Jul/2023 12:14:22] "POST /chat HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:14:33] "POST /chat HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:15:36] "GET / HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:16:38] "GET / HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:16:42] "POST /chat HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:16:45] "POST /chat HTTP/1.1" 200 -
147.8.178.199 - - [26/Jul/2023 12:16:58] "POST /chat HTTP/1.1" 200 -
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [26/Jul/2023 20:46:09] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17077 tokens (777 in the messages, 16300 in the completion). Please reduce the length of the messages or completion.
147.8.179.195 - - [26/Jul/2023 20:46:20] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:46:20] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [26/Jul/2023 20:46:27] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17077 tokens (777 in the messages, 16300 in the completion). Please reduce the length of the messages or completion.
147.8.179.195 - - [26/Jul/2023 20:47:18] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:47:34] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:47:34] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [26/Jul/2023 20:47:42] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18403 tokens (2103 in the messages, 16300 in the completion). Please reduce the length of the messages or completion.
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [26/Jul/2023 20:49:19] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:49:32] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:49:45] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:50:10] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:50:42] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16499 tokens (2499 in the messages, 14000 in the completion). Please reduce the length of the messages or completion.
147.8.179.195 - - [26/Jul/2023 20:52:09] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:52:18] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:52:31] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:53:00] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:53:34] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [26/Jul/2023 20:54:45] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 09:46:09] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 09:47:31] "[31m[1mGET /clear_history HTTP/1.1[0m" 405 -
147.8.179.195 - - [27/Jul/2023 09:48:09] "[31m[1mGET /clear_history HTTP/1.1[0m" 405 -
147.8.179.195 - - [27/Jul/2023 09:48:20] "[33mGET /404 HTTP/1.1[0m" 404 -
147.8.179.195 - - [27/Jul/2023 09:48:25] "[33mGET /405 HTTP/1.1[0m" 404 -
147.8.179.195 - - [27/Jul/2023 09:48:42] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 09:49:05] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 09:50:08] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 09:50:34] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 11:01:12] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [27/Jul/2023 15:24:38] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:04:14] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:04:15] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [28/Jul/2023 10:04:22] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16493 tokens (2493 in the messages, 14000 in the completion). Please reduce the length of the messages or completion.
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [28/Jul/2023 10:06:43] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:06:51] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:07:20] "POST /chat HTTP/1.1" 200 -
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/config.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [28/Jul/2023 10:09:07] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:09:33] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:10:15] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
Traceback (most recent call last):
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2213, in __call__
    return self.wsgi_app(environ, start_response)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2193, in wsgi_app
    response = self.handle_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 2190, in wsgi_app
    response = self.full_dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1486, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask_cors/extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1484, in full_dispatch_request
    rv = self.dispatch_request()
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/flask/app.py", line 1469, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/app.py", line 91, in get_chat_response
    answer = chat(user_id, prompt)
  File "/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py", line 26, in chat
    response = openai.ChatCompletion.create(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/autofs/bal1-1/xzhang/anaconda3/envs/flask/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16488 tokens (3188 in the messages, 13300 in the completion). Please reduce the length of the messages or completion.
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/config.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/chatbot.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [28/Jul/2023 10:16:12] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:17:00] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:17:15] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:17:40] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:17:49] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:18:22] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:18:22] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [28/Jul/2023 10:18:49] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:18:50] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [28/Jul/2023 10:18:57] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:19:29] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:20:12] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:23:27] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 10:23:27] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [28/Jul/2023 10:23:34] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 11:10:59] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [28/Jul/2023 11:10:59] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/config.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
 * Detected change in '/autofs/bal1-1/xzhang/DevSoftDir/Flask/MulMarker/config.py', reloading
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 571-193-735
147.8.179.195 - - [01/Aug/2023 19:38:12] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [01/Aug/2023 19:38:12] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [01/Aug/2023 19:39:19] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [01/Aug/2023 19:39:19] "GET /static/Test_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [01/Aug/2023 19:39:19] "GET /static/Test_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [01/Aug/2023 19:39:19] "GET /static/Test_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:00:29] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:00:29] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [03/Aug/2023 12:01:17] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:02:15] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:07:23] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:07:34] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:08:22] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:08:22] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [03/Aug/2023 12:08:28] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:09:04] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 12:31:33] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:05:48] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:06:21] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:06:34] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:08:53] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:09:38] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:09:38] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [03/Aug/2023 15:09:45] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:10:41] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:11:27] "[33mGET /clear HTTP/1.1[0m" 404 -
147.8.179.195 - - [03/Aug/2023 15:11:35] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:13:20] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:16:50] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:17:51] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:22:54] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 15:33:01] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 17:07:10] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 17:07:11] "GET /static/BRCA_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 17:07:11] "GET /static/BRCA_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 17:07:11] "GET /static/BRCA_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [03/Aug/2023 17:17:48] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:54:08] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:54:08] "GET /static/BRCA_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:54:08] "GET /static/BRCA_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:54:08] "GET /static/BRCA_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:55:30] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:55:53] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:55:53] "GET /static/BRCA_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:55:53] "GET /static/BRCA_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:55:53] "GET /static/BRCA_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:58:23] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:58:51] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:58:51] "GET /static/BRCA_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:58:51] "GET /static/BRCA_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:58:51] "GET /static/BRCA_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:59:21] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:59:51] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:59:51] "GET /static/BRCA_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:59:51] "GET /static/BRCA_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 11:59:51] "GET /static/BRCA_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 12:03:32] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 12:04:02] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 12:04:02] "GET /static/BRCA_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 12:04:02] "GET /static/BRCA_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [04/Aug/2023 12:04:02] "GET /static/BRCA_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 10:50:37] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 10:50:37] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [08/Aug/2023 10:53:52] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 10:57:18] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 10:58:04] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 11:05:30] "POST / HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 11:05:30] "GET /static/wawa_train_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 11:05:30] "GET /static/wawa_total_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [08/Aug/2023 11:05:30] "GET /static/wawa_test_survival_plot.png HTTP/1.1" 200 -
147.8.179.195 - - [12/Aug/2023 11:42:47] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [12/Aug/2023 11:42:54] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [12/Aug/2023 11:42:54] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
147.8.179.195 - - [12/Aug/2023 11:43:26] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [12/Aug/2023 11:44:57] "POST /chat HTTP/1.1" 200 -
147.8.179.195 - - [14/Aug/2023 10:26:29] "GET / HTTP/1.1" 200 -
147.8.179.195 - - [14/Aug/2023 10:26:29] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
